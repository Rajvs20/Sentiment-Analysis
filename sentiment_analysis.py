# -*- coding: utf-8 -*-
"""CS779-Sentiment-Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vdVLGp1iru2znivllTfXl_iLSlIbWtc8
"""

!pip install torchtext==0.6.0

from google.colab import drive
drive.mount('/content/drive')

# Load the spacy English tokenizer
spacy_english = spacy.load("en_core_web_sm")

import torch
from torchtext.data import Field, LabelField, BucketIterator, TabularDataset
import spacy
import re

def decontract(sentence):
  sentence=sentence.split()
  sentence=' '.join(sentence)
  sentence=re.sub(r"won't",'will not',sentence)
  sentence=re.sub(r"can't",'can not',sentence)
  sentence=re.sub(r"n\'t",' not',sentence)
  sentence=re.sub(r"\'re",' are',sentence)
  sentence=re.sub(r"\'d",' would',sentence)
  sentence=re.sub(r"\'ll",' will',sentence)
  sentence=re.sub(r"\'s",' is',sentence)
  sentence=re.sub(r"\'m",' am',sentence)
  sentence=re.sub(r"\'ve",' have',sentence)
  return sentence

def cleanEng(x):
  x=str(x)
  x=x.lower()
  x=re.sub(r'[^a-z0-9]+',' ',x)
  x=re.sub(' +', ' ',x) #removing extra spaces 
  if x and x[-1]==' ':
    x=x[:-1]
  x=x.strip()
  return x
  
def tokenize_english(text):
    text = decontract(text)
    text = cleanEng(text)
    return [token.text for token in spacy_english.tokenizer(text)]

# Define the fields
TEXT = Field(sequential=True, tokenize=tokenize_english, lower=True, init_token="<sos>", eos_token="<eos>", include_lengths = True)
LABEL = LabelField(sequential=False, batch_first=True)
ID = Field(use_vocab=False, batch_first=True)

path = '/content/drive/MyDrive/NLP_stuff/Sentiment_analysis/train.csv'
# load the CSV data into a TabularDataset
data = TabularDataset(
    path=path,
    format='csv', 
    fields=[('id', None),("text", TEXT), ("label", LABEL)], 
    skip_header=True
)

# split the dataset into train and validation sets
train_data, test_data = data.split(split_ratio=0.9, stratified=False)

# Build the vocabulary
TEXT.build_vocab(train_data, max_size=25000, min_freq=3, vectors = "glove.6B.100d", unk_init = torch.Tensor.normal_)
LABEL.build_vocab(train_data)

print(len(train_data))

import torch
from torchtext import data

BATCH_SIZE = 512

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_iterator, test_iterator = data.BucketIterator.splits(
    (train_data, test_data), 
    batch_size = BATCH_SIZE,
    sort_within_batch = True,
    sort_key=lambda x: len(x.text), 
    device = device)

import torch.nn as nn

class LSTM_RNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, 
                 bidirectional, dropout, pad_idx):
        
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)
        
        self.rnn = nn.LSTM(embedding_dim, 
                           hidden_dim, 
                           num_layers=n_layers, 
                           bidirectional=bidirectional, 
                           dropout=dropout)
        
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, text, text_lengths):        
        embedded = self.dropout(self.embedding(text))
        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))
        packed_output, (hidden, cell) = self.rnn(packed_embedded)
        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)
        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))
        return self.fc(hidden)

INPUT_DIM = len(TEXT.vocab)
EMBEDDING_DIM = 100
HIDDEN_DIM = 400
OUTPUT_DIM = 3
N_LAYERS = 2
BIDIRECTIONAL = True
DROPOUT = 0.5
PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]

model = LSTM_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)

pretrained_embeddings = TEXT.vocab.vectors
model.embedding.weight.data.copy_(pretrained_embeddings)
UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]
model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)
model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)

import torch.optim as optim

optimizer = optim.Adam(model.parameters(), lr=0.0001)

criterion = nn.CrossEntropyLoss().to(device)

model = model.to(device)

def categorical_accuracy(preds, y):
    top_pred = preds.argmax(1, keepdim = True)
    correct = top_pred.eq(y.view_as(top_pred)).sum()
    acc = correct.float() / y.shape[0]
    return acc

def train(model, iterator, optimizer, criterion):
    
    epoch_loss = 0
    epoch_acc = 0
    
    model.train()
    
    for batch in iterator:
        
        optimizer.zero_grad()
        
        
        text, text_lengths = batch.text
        
        predictions = model(text, text_lengths).squeeze(1)

        loss = criterion(predictions, batch.label)
        
        acc = categorical_accuracy(predictions, batch.label)
        
        loss.backward()
        
        optimizer.step()
        
        epoch_loss += loss.item()
        epoch_acc += acc.item()
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

def evaluate(model, iterator, criterion):
    
    epoch_loss = 0
    epoch_acc = 0
    
    model.eval()
    
    with torch.no_grad():
    
        for batch in iterator:
            text, text_lengths = batch.text
            
            predictions = model(text, text_lengths).squeeze(1)
            
            loss = criterion(predictions, batch.label)
            
            acc = categorical_accuracy(predictions, batch.label)

            epoch_loss += loss.item()
            epoch_acc += acc.item()
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

import time

def timePerEpoch(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

N_EPOCHS = 5

# best_valid_loss = 999999

for epoch in range(N_EPOCHS):

    start_time = time.time()
    
    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)
    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)
    
    end_time = time.time()

    epoch_mins, epoch_secs = timePerEpoch(start_time, end_time)

model.load_state_dict(torch.load('model.pt'))

test_loss, test_acc = evaluate(model, test_iterator, criterion)

print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')

def predict_class(model, sentence):
    model.eval()
    tokenized = tokenize_english(sentence)
    indexed = [TEXT.vocab.stoi[t] for t in tokenized]
    length = [len(indexed)]
    tensor = torch.LongTensor(indexed).to(device)
    tensor = tensor.unsqueeze(1)
    length_tensor = torch.LongTensor(length)
    preds = model(tensor, length_tensor)
    max_preds = preds.argmax(dim = 1)
    return max_preds.item()

import csv
import chardet

FILENAME = '/content/drive/MyDrive/NLP_stuff/Sentiment_analysis/test.csv'
with open(FILENAME, 'rb') as f:
    result = chardet.detect(f.read())

with open(FILENAME, newline='', encoding=result['encoding']) as csvfile:
    csv_reader = csv.reader(csvfile, delimiter=',', quotechar='"')

    # Skip the header row
    next(csv_reader)

    f_open = open("answer1.txt","w")
    for row in csv_reader:
        pred = LABEL.vocab.itos[predict_class(model, str(row[1]))]
        s= str(pred)
        s+='\n'
        f_open.write(s)
    f_open.close()

